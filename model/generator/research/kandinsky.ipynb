{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'diffusers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/home/prof_evaluation/MASSter/model/generator/research/kandinsky.ipynb Cell 1\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22434f44227d/home/prof_evaluation/MASSter/model/generator/research/kandinsky.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msys\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22434f44227d/home/prof_evaluation/MASSter/model/generator/research/kandinsky.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdiffusers\u001b[39;00m \u001b[39mimport\u001b[39;00m KandinskyV22Pipeline, KandinskyV22PriorPipeline\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22434f44227d/home/prof_evaluation/MASSter/model/generator/research/kandinsky.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22434f44227d/home/prof_evaluation/MASSter/model/generator/research/kandinsky.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mPIL\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'diffusers'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from diffusers import KandinskyV22Pipeline, KandinskyV22PriorPipeline\n",
    "import torch\n",
    "import PIL\n",
    "import torch\n",
    "from diffusers.utils import load_image\n",
    "from torchvision import transforms\n",
    "from transformers import CLIPVisionModelWithProjection\n",
    "from diffusers.models import UNet2DConditionModel\n",
    "import numpy as np\n",
    "\n",
    "DEVICE = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_encoder = CLIPVisionModelWithProjection.from_pretrained(\n",
    "    'kandinsky-community/kandinsky-2-2-prior',\n",
    "    subfolder='image_encoder'\n",
    ").half().to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet = UNet2DConditionModel.from_pretrained(\n",
    "    'kandinsky-community/kandinsky-2-2-decoder',\n",
    "    subfolder='unet'\n",
    ").half().to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...: 100%|██████████| 6/6 [00:14<00:00,  2.47s/it]\n"
     ]
    }
   ],
   "source": [
    "prior = KandinskyV22PriorPipeline.from_pretrained(\n",
    "    'kandinsky-community/kandinsky-2-2-prior',\n",
    "    image_encoder=image_encoder,\n",
    "    torch_dtype=torch.float16\n",
    ").to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...: 100%|██████████| 3/3 [00:00<00:00,  4.34it/s]\n"
     ]
    }
   ],
   "source": [
    "decoder = KandinskyV22Pipeline.from_pretrained(\n",
    "    'kandinsky-community/kandinsky-2-2-decoder',\n",
    "    unet=unet,\n",
    "    torch_dtype=torch.float16\n",
    ").to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:07<00:00, 32.71it/s]\n",
      "100%|██████████| 250/250 [00:08<00:00, 30.18it/s]\n",
      "100%|██████████| 250/250 [00:18<00:00, 13.30it/s]\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "negative_prior_prompt ='lowres, text, error, cropped, worst quality, low quality, jpeg artifacts, ugly, duplicate, morbid, mutilated, out of frame, extra fingers, mutated hands, poorly drawn hands, poorly drawn face, mutation, deformed, blurry, dehydrated, bad anatomy, bad proportions, extra limbs, cloned face, disfigured, gross proportions, malformed limbs, missing arms, missing legs, extra arms, extra legs, fused fingers, too many fingers, long neck, username, watermark, signature'\n",
    "negative_prior_prompt = 'low quality, bad quality'\n",
    "\n",
    "img_emb = prior(\n",
    "    prompt='big bag with entering room with clown with me, interstring, atmospheric, many words, many cocks, many rocks, many boobs, many people, many kicks, disney, pixar, ea sports, dotnet, c++, clownish, british, european union, programmer, complicated, overall, international, music, pitbull, dogs, cats, rats, racoons, nature, family, fast and furious like, century, film, am i too clownish right now, is it gonna end sometime, what the hell man stop please already you clown stop stop',\n",
    "    num_inference_steps=250,\n",
    "    num_images_per_prompt=1\n",
    ")\n",
    "\n",
    "negative_emb = prior(\n",
    "    prompt=negative_prior_prompt,\n",
    "    num_inference_steps=250,\n",
    "    num_images_per_prompt=1\n",
    ")\n",
    "\n",
    "images = decoder(\n",
    "    image_embeds=img_emb.image_embeds,\n",
    "    negative_image_embeds=negative_emb.image_embeds,\n",
    "    num_inference_steps=250,\n",
    "    guidance_scale=4.0,\n",
    "    height=380,\n",
    "    width=720)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PIL.Image.Image"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(images.images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PIL.Image.Image"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PIL.Image.Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images.images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-large\").to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_desc(processor, model, filename):\n",
    "    raw_image = Image.open(filename).convert('RGB')\n",
    "\n",
    "    # conditional image captioning\n",
    "    text = \"a photography of\"\n",
    "    inputs = processor(raw_image, text, return_tensors=\"pt\").to('cuda:0')\n",
    "\n",
    "    out = model.generate(**inputs)\n",
    "    print(processor.decode(out[0], skip_special_tokens=True))\n",
    "\n",
    "    # unconditional image captioning\n",
    "    inputs = processor(raw_image, return_tensors=\"pt\").to('cuda:0')\n",
    "\n",
    "    out = model.generate(**inputs)\n",
    "    print(processor.decode(out[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prof_evaluation/miniconda3/envs/aino-latest/lib/python3.9/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a photography of a cat playing with a wooden table with bowls of food\n",
      "there is a cat that is playing with a wooden table\n"
     ]
    }
   ],
   "source": [
    "generate_desc(processor, model, 'ph.jpg')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hackaton",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
